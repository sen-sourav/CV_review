{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Object Detection with Transformers \n",
    "\n",
    "(https://arxiv.org/abs/2005.12872)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output positional encodings (object queries) \n",
    "\n",
    "\"Output positional encodings are required and cannot be removed, so we experiment with either passing them once at decoder input or adding to queries at every decoder attention layer.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"arXiv_2005_12872/fig2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"arXiv_2005_12872/fig3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I think the purpose of the output position encoding is to introduce some kind of spatial masking or mutual exclusiveness between the N attention maps each of which is supposed to include a different object. (see attention maps in Fig. 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"arXiv_2005_12872/fig8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is why the output position encodings/object queries need to be trained/learned.\n",
    "So, the N object queries are trying to be mutually exclusive (should be plotted and verified).\n",
    "We can perhaps try using sine, cosine position encodings with their frequency as a function of i- encoding dim and n (n-th object query out of N object queries), so that the object queries are orthogonal to each other. That way the encoder will learn to map differnt objects (and non-objects) to one of the N orthogonal basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looking at: \n",
    "- https://arxiv.org/pdf/1802.05751.pdf (for 2D version used in the paper)\n",
    "- https://arxiv.org/pdf/1904.07418.pdf\n",
    "- https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding\n",
    "- http://jalammar.github.io/illustrated-transformer/\n",
    "- https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://arxiv.org/abs/2005.12872\n",
    "\n",
    "\"Specifically, for both spatial coordinates of each embedding we independently used d/2 sine and cosine functions with different frequencies. We then concatenate them to get the final d channel positional encoding.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-CNN (“Region-based CNN”) family\n",
    "\n",
    "\n",
    "\n",
    "| Model | Goal | Resources |\n",
    "|-------|------|-----------|\n",
    "| R-CNN | Object recognition | [[paper](https://arxiv.org/abs/1311.2524)] [[code](https://github.com/rbgirshick/rcnn)]|\n",
    "| Fast R-CNN | Object recognition | [[paper](https://arxiv.org/abs/1504.08083)] [[code](https://github.com/rbgirshick/fast-rcnn)]|\n",
    "| Faster R-CNN | Object recognition | [[paper](https://arxiv.org/abs/1506.01497)] [[code](https://github.com/rbgirshick/py-faster-rcnn)]|\n",
    "| Mask R-CNN | Image Segmentation | [[paper](https://arxiv.org/abs/1703.06870)] [[code](https://github.com/CharlesShang/FastMaskRCNN)]|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading up: https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- select ~2000 region candidates (RoI) using selective search (to read: https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html#selective-search)\n",
    "    * after selective search, unique boxes for objects are chosen using non-maximum suppression\n",
    "- warp each box for fixed input size to run a ConvNet\n",
    "- ConvNet features classified using an SVM (is SVM not a specific(binary) case as softmaax?) into object classes.\n",
    "- At the end offset correction is made for the predicted boxes and the ground truth boxes using regression when the predicted boxes have a descent overlap with the ground truth boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - To avoid running ConvNet on each box separetely,ConvNet is first run on the entire image.\n",
    "    - Then object (RoI) are chosen on the ConvNet output (activation map) using selective search.\n",
    "        * this is the bottleneck step now\n",
    "    - then RoI warped using RoI pooling and fed to FCN to produce feature vector\n",
    "    - feature vector classified using softmax into K+1 (extra one for bkg) object classes and box regression to adjusting to predicted box to match the ground truth box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the selective search object box step which is bottleneck, this trains a network and replaces selective search.\n",
    "\n",
    "rest is the same as Fast R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN\n",
    "\n",
    "does image seg inside object boxes as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://lilianweng.github.io/lil-log/assets/images/rcnn-family-summary.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
